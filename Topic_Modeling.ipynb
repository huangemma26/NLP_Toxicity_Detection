{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling \n",
    "\n",
    "This notebook contains topic modeling and analysis on Reddit posts across a variety of subreddits. In this way, clear differences and similarities can be seen between different groups on Reddit.\n",
    "\n",
    "Section Breakdown:\n",
    "1. Importing Libraries and Data\n",
    "2. Preliminary Text Cleaning\n",
    "3. Tokenize the Posts\n",
    "4. LDA Topic Modeling Using Word Counts\n",
    "5. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Libraries and Data\n",
    "Data obtained from the [BigQuery Reddit post dataset](https://bigquery.cloud.google.com/table/fh-bigquery:reddit_posts.2017_09?pli=1). Subreddits chosen were all text based dealing with similar themes so comparisons could be made more clearly. These subreddits include:\n",
    "* r/incels\n",
    "* r/legaladvice\n",
    "* r/redpill\n",
    "* r/relationships\n",
    "* r/twoxchromosomes\n",
    "* r/confessions\n",
    "\n",
    "r/incels and r/redpill are included to give a benchmark for toxicity, as these communities contain much more hate speech than the majority of subreddits. Ultimately, we look to create a measure to identify similar hate speech in other environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "# Pandas and Numpy to format data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text cleaning\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Tolkenize the corpuses \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "#Models\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF,PCA\n",
    "from gensim import corpora, models, similarities, matutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r/incels post data\n",
    "incels1 = pd.read_csv(\"../posts/incels.csv\")\n",
    "incels2 = pd.read_csv(\"../posts/incels2.csv\")\n",
    "incels3 = pd.read_csv(\"../posts/incels3.csv\")\n",
    "incels = pd.concat([incels1, incels2, incels3], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r/legaladvice post data\n",
    "legal1 = pd.read_csv(\"../posts/legaladvice.csv\")\n",
    "legal2 = pd.read_csv(\"../posts/legaladvice2.csv\")\n",
    "legal3 = pd.read_csv(\"../posts/legaladvice3.csv\")\n",
    "legal = pd.concat([legal1, legal2, legal3], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r/redpill post data\n",
    "redpill1 = pd.read_csv(\"../posts/redpill.csv\")\n",
    "redpill2 = pd.read_csv(\"../posts/redpill2.csv\")\n",
    "redpill3 = pd.read_csv(\"../posts/redpill3.csv\")\n",
    "redpill = pd.concat([redpill1, redpill2, redpill3], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r/relationships post data\n",
    "relationships1 = pd.read_csv(\"../posts/relationships.csv\")\n",
    "relationships2 = pd.read_csv(\"../posts/relationships2.csv\")\n",
    "relationships3 = pd.read_csv(\"../posts/relationships3.csv\")\n",
    "relationships = pd.concat([relationships1, relationships2, relationships3], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r/twoxchromosomes post data\n",
    "twox1 = pd.read_csv(\"../posts/twox.csv\")\n",
    "twox2 = pd.read_csv(\"../posts/twox2.csv\")\n",
    "twox3 = pd.read_csv(\"../posts/twox3.csv\")\n",
    "twox = pd.concat([twox1, twox2, twox3], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r/confessions post data\n",
    "confessions1 = pd.read_csv(\"../posts/confessions.csv\")\n",
    "confessions2 = pd.read_csv(\"../posts/confessions2.csv\")\n",
    "confessions3 = pd.read_csv(\"../posts/confessions3.csv\")\n",
    "confessions = pd.concat([confessions1, confessions2, confessions3], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data into one dataframe\n",
    "posts = pd.concat([incels, legal, redpill, relationships, twox, confessions], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preliminary Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function to remove urls from the corpuses\n",
    "def remove_url(item):\n",
    "    item2 = item\n",
    "    item2 = re.sub(r\"http\\S+\", \"\", item2)\n",
    "    item2 = re.sub(r\"www\\S+\", \"\", item2)\n",
    "    return item2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove posts that are too short, where topics would be more difficult to identify\n",
    "def remove_short_posts(item):\n",
    "    item = str(item)\n",
    "    if len(item) > 300:\n",
    "        return item\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove posts that were removed or deleted\n",
    "posts = posts[~posts['selftext'].isin(['[removed]','[deleted]'])].reset_index(drop=True)\n",
    "# Apply the earlier function to remove short posts\n",
    "posts['selftext'] = posts['selftext'].apply(remove_short_posts)\n",
    "# Remove all the None type posts\n",
    "posts.dropna(subset=['selftext'], inplace = True)\n",
    "# Reset the index\n",
    "posts = posts.reset_index(drop=True)\n",
    "# Apply the earlier function to remove urls\n",
    "posts['selftext'] = posts['selftext'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tokenize the Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stop words including Reddit specific slang\n",
    "other_stop_words = ['incel','chad', 'alpha', 'beta',\n",
    "                    've','gt','amp','nbsp','rp','pm', 'ampnbsp','th','aa',\n",
    "                    'adam','john',\n",
    "                    'really', 'just',\n",
    "                    'b','c','d','e','f','g','h','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
    "                    '’','”',\"“\"\n",
    "                   ]\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(other_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Custom tokenizer in order to lemmatize words\n",
    "def custom_tokenizer(text):\n",
    "\n",
    "    # remove punctuation\n",
    "    remove_punct = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(remove_punct)\n",
    "\n",
    "    # remove digits and convert to lower case\n",
    "    remove_digits = str.maketrans('', '', string.digits)\n",
    "    text = text.lower().translate(remove_digits)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words2 = stop_words\n",
    "    tokens_stop = [y for y in tokens if y not in stop_words2]\n",
    "\n",
    "    # stem\n",
    "    lem = WordNetLemmatizer()\n",
    "    tokens_lem = [lem.lemmatize(y) for y in tokens_stop] \n",
    "\n",
    "    return tokens_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize with CountVectorizer\n",
    "count_vect = CountVectorizer(tokenizer=custom_tokenizer, max_df=0.8, min_df=0.025)\n",
    "count_vect.fit(posts['selftext'])\n",
    "# Combine some common terms\n",
    "count_vect.vocabulary_['man'] = count_vect.vocabulary_['men']\n",
    "count_vect.vocabulary_['kid'] = count_vect.vocabulary_['child']\n",
    "# Get token counts per post\n",
    "counts = count_vect.transform(posts['selftext']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize with TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=custom_tokenizer, max_df=0.8, min_df=0.025)\n",
    "tfidf_vect.fit(posts['selftext'])\n",
    "# Combine some common terms\n",
    "tfidf_vect.vocabulary_['man'] = tfidf_vect.vocabulary_['men']\n",
    "tfidf_vect.vocabulary_['kid'] = tfidf_vect.vocabulary_['child']\n",
    "# Get token tfidf per post\n",
    "tfidf = tfidf_vect.transform(posts['selftext']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LDA Topic Modeling Using Word Counts\n",
    "\n",
    "Note: Unable to use Tfidf to train LDA topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "corpus = matutils.Sparse2Corpus(counts)\n",
    "id2word = dict((v, k) for k, v in count_vect.vocabulary_.items())\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=20, id2word=id2word, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.218*\"im\" + 0.086*\"ive\" + 0.053*\"dont\" + 0.049*\"he\" + 0.034*\"know\" + 0.031*\"like\" + 0.029*\"shes\" + 0.020*\"id\" + 0.016*\"thats\" + 0.015*\"sure\"'),\n",
       " (1,\n",
       "  '0.105*\"work\" + 0.069*\"job\" + 0.059*\"company\" + 0.033*\"hour\" + 0.029*\"business\" + 0.029*\"working\" + 0.027*\"time\" + 0.025*\"day\" + 0.022*\"week\" + 0.022*\"employee\"'),\n",
       " (2,\n",
       "  '0.208*\"house\" + 0.105*\"home\" + 0.062*\"room\" + 0.046*\"door\" + 0.035*\"property\" + 0.033*\"water\" + 0.025*\"living\" + 0.022*\"leave\" + 0.022*\"come\" + 0.021*\"clean\"'),\n",
       " (3,\n",
       "  '0.042*\"want\" + 0.042*\"dont\" + 0.039*\"like\" + 0.031*\"know\" + 0.028*\"feel\" + 0.026*\"thing\" + 0.023*\"make\" + 0.020*\"people\" + 0.019*\"think\" + 0.016*\"say\"'),\n",
       " (4,\n",
       "  '0.341*\"friend\" + 0.062*\"people\" + 0.050*\"group\" + 0.037*\"social\" + 0.031*\"video\" + 0.031*\"game\" + 0.029*\"picture\" + 0.023*\"best\" + 0.023*\"facebook\" + 0.021*\"close\"'),\n",
       " (5,\n",
       "  '0.033*\"relationship\" + 0.027*\"time\" + 0.027*\"feel\" + 0.025*\"like\" + 0.021*\"want\" + 0.020*\"friend\" + 0.019*\"year\" + 0.018*\"love\" + 0.016*\"thing\" + 0.016*\"boyfriend\"'),\n",
       " (6,\n",
       "  '0.042*\"said\" + 0.037*\"told\" + 0.033*\"didnt\" + 0.022*\"got\" + 0.020*\"went\" + 0.020*\"did\" + 0.019*\"asked\" + 0.017*\"day\" + 0.015*\"know\" + 0.015*\"wanted\"'),\n",
       " (7,\n",
       "  '0.164*\"child\" + 0.074*\"wife\" + 0.067*\"husband\" + 0.049*\"married\" + 0.041*\"year\" + 0.032*\"marriage\" + 0.031*\"daughter\" + 0.031*\"baby\" + 0.030*\"son\" + 0.024*\"divorce\"'),\n",
       " (8,\n",
       "  '0.127*\"woman\" + 0.100*\"men\" + 0.024*\"female\" + 0.022*\"life\" + 0.021*\"male\" + 0.017*\"people\" + 0.016*\"deposit\" + 0.016*\"look\" + 0.013*\"like\" + 0.012*\"youre\"'),\n",
       " (9,\n",
       "  '0.064*\"year\" + 0.058*\"month\" + 0.037*\"time\" + 0.035*\"week\" + 0.031*\"day\" + 0.029*\"ago\" + 0.028*\"new\" + 0.016*\"period\" + 0.013*\"old\" + 0.012*\"plan\"'),\n",
       " (10,\n",
       "  '0.053*\"police\" + 0.052*\"court\" + 0.040*\"ticket\" + 0.034*\"case\" + 0.027*\"record\" + 0.027*\"charge\" + 0.027*\"report\" + 0.026*\"test\" + 0.025*\"license\" + 0.022*\"drug\"'),\n",
       " (11,\n",
       "  '0.028*\"like\" + 0.025*\"time\" + 0.021*\"shit\" + 0.018*\"dog\" + 0.018*\"night\" + 0.017*\"fucking\" + 0.017*\"look\" + 0.016*\"day\" + 0.015*\"fuck\" + 0.012*\"face\"'),\n",
       " (12,\n",
       "  '0.137*\"school\" + 0.052*\"college\" + 0.051*\"year\" + 0.049*\"don\" + 0.042*\"class\" + 0.040*\"high\" + 0.040*\"doctor\" + 0.031*\"student\" + 0.026*\"pain\" + 0.025*\"university\"'),\n",
       " (13,\n",
       "  '0.072*\"money\" + 0.061*\"pay\" + 0.032*\"lawyer\" + 0.031*\"account\" + 0.026*\"card\" + 0.026*\"paid\" + 0.025*\"contract\" + 0.023*\"letter\" + 0.020*\"agreement\" + 0.020*\"credit\"'),\n",
       " (14,\n",
       "  '0.027*\"question\" + 0.024*\"im\" + 0.022*\"thanks\" + 0.021*\"advice\" + 0.019*\"post\" + 0.018*\"thank\" + 0.016*\"edit\" + 0.015*\"know\" + 0.015*\"right\" + 0.015*\"help\"'),\n",
       " (15,\n",
       "  '0.176*\"girl\" + 0.123*\"sex\" + 0.119*\"guy\" + 0.046*\"sexual\" + 0.033*\"pill\" + 0.018*\"having\" + 0.017*\"red\" + 0.013*\"party\" + 0.013*\"experience\" + 0.013*\"partner\"'),\n",
       " (16,\n",
       "  '0.157*\"car\" + 0.062*\"insurance\" + 0.026*\"driving\" + 0.026*\"damage\" + 0.025*\"vehicle\" + 0.023*\"accident\" + 0.022*\"driver\" + 0.020*\"officer\" + 0.016*\"hit\" + 0.016*\"drive\"'),\n",
       " (17,\n",
       "  '0.078*\"phone\" + 0.056*\"message\" + 0.051*\"sent\" + 0.050*\"text\" + 0.050*\"email\" + 0.047*\"contact\" + 0.039*\"number\" + 0.039*\"send\" + 0.033*\"day\" + 0.028*\"check\"'),\n",
       " (18,\n",
       "  '0.081*\"apartment\" + 0.064*\"lease\" + 0.056*\"rent\" + 0.047*\"roommate\" + 0.045*\"landlord\" + 0.032*\"month\" + 0.027*\"fee\" + 0.026*\"moved\" + 0.024*\"tenant\" + 0.022*\"notice\"'),\n",
       " (19,\n",
       "  '0.081*\"mom\" + 0.070*\"family\" + 0.062*\"parent\" + 0.058*\"dad\" + 0.051*\"mother\" + 0.049*\"sister\" + 0.041*\"brother\" + 0.030*\"father\" + 0.025*\"year\" + 0.020*\"life\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the topics\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics seem to be fairly coherent. Topic labels:\n",
    "0. Other\n",
    "1. Work\n",
    "2. House Ownership\n",
    "3. Insecurity\n",
    "4. Friendship\n",
    "5. Romantic Relationships\n",
    "6. Seeking Advice\n",
    "7. Marriage\n",
    "8. Gender Issues\n",
    "9. Time Periods\n",
    "10. Illegal Activity\n",
    "11. Crude Sex\n",
    "12. College\n",
    "13. Lawyer\n",
    "14. Thankful for Advice\n",
    "15. Respectful Sex\n",
    "16. Car Legal Issues\n",
    "17. Texting\n",
    "18. Apartments\n",
    "19. Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.11682687),\n",
       "  (3, 0.34772292),\n",
       "  (8, 0.122964025),\n",
       "  (9, 0.07831175),\n",
       "  (12, 0.12128804),\n",
       "  (13, 0.10906896),\n",
       "  (15, 0.09241391)],\n",
       " [(6, 0.49987704), (11, 0.455123)],\n",
       " [(3, 0.38820687),\n",
       "  (8, 0.25135508),\n",
       "  (11, 0.19065842),\n",
       "  (15, 0.10549299),\n",
       "  (19, 0.04009305)],\n",
       " [(0, 0.21043836),\n",
       "  (3, 0.35669893),\n",
       "  (9, 0.17327507),\n",
       "  (12, 0.10357035),\n",
       "  (14, 0.048274882),\n",
       "  (15, 0.066565976)],\n",
       " [(0, 0.086266175),\n",
       "  (8, 0.07941179),\n",
       "  (9, 0.08448296),\n",
       "  (11, 0.5931004),\n",
       "  (13, 0.03766228),\n",
       "  (14, 0.06181989),\n",
       "  (15, 0.046423126)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict topics for each post\n",
    "lda_corpus = lda[corpus]\n",
    "lda_docs = [doc for doc in lda_corpus]\n",
    "# View example posts to get a sense of distributions and topics accuracy\n",
    "lda_docs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find predicted topic for each post\n",
    "lda_topics = []\n",
    "for lis in lda_docs:\n",
    "    topic = max(lis,key=lambda item:item[1])[0]\n",
    "    lda_topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the data frame with only relevant info\n",
    "new_posts = posts[['subreddit','score','selftext']]\n",
    "# Add predicted topic for each post\n",
    "new_posts['lda_topic'] = lda_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Analysis\n",
    "Discoveries from the topics found through LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which topics score the highest and lowest on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lda_topic\n",
       "16     16.755669\n",
       "18     17.338235\n",
       "9      17.922013\n",
       "0      20.177503\n",
       "13     20.320028\n",
       "17     20.583333\n",
       "2      23.339080\n",
       "5      29.741906\n",
       "15     32.690141\n",
       "1      34.226852\n",
       "10     35.845679\n",
       "14     41.523262\n",
       "19     54.977716\n",
       "8      62.199422\n",
       "3      66.156520\n",
       "11     89.198892\n",
       "4     108.353448\n",
       "6     111.297777\n",
       "7     115.892202\n",
       "12    159.488789\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_posts.groupby( ['lda_topic'] )['score'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 16, car legal issues, scores the lowest while topic 12, college, scores the highest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do subreddits compare with each other? \n",
    "\n",
    "<img src=\"topic_distributions.png\" style=\"width: 500px;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After graphing the distribution of the topics (where each color in the graph above represents a different topic), it's clear that r/incels and r/theredpill share a very similar breakdown. This is useful for identifying future toxic communities. r/confessions and r/twoxchromosomes also appear to similar, while legal advice has many more topics but no predominant ones and r/relationships largely focuses on one topic (5: Romantic Relationships).\n",
    "\n",
    "In addition, the pink topic (11: Crude Sex) and purple topic (3: Insecurity) are the predominant ones for the toxic subreddits, while they are minor occurrences in the other subreddits. This means that identifying these topics can give an indication for where hate speak is occurring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
